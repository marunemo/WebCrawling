웹 크롤링(Web Crawling)
===

    소프트웨어 프로그램을 통해 웹 사이트에 자동으로 접속하여 해당 데이터를 얻는 기술

## 웹 크롤링의 사용

보통의 경우라면, 각종 사이트에서의 검색 엔진이 사용한다.
웹 크롤링을 통해 얻어진 데이터에서 검색 알고리즘을 적용하여 사용자의 검색 질의에 대한 응답으로 관련 링크를 제공하는 것이 주된 사용이다.
조사에 따르면, 전체 인터넷의 40~70% 가량이 검색을 위해 인덱싱된다고 한다.
이는 수십억 개의 웹페이지에 해당하는 정보량이다.

## 웹 크롤러의 작동 원리

seed나 크롤링할 URL 목록을 받는 것으로 시작하여, 해당 URL의 웹사이트를 크롤링하여 데이터를 추출해낸다.
해당 웹사이트를 크롤링하는 중에 다른 하이퍼링크를 찾아내면, 크롤링할 URL 목록에 해당 링크를 추가한다.
이러한 과정이 반복되면 그 목록의 항목은 무한히 늘어날 수 있다.
때문에, 크롤링할 URL의 목록을 선택하기 위해, 대상, 목적, 업데이트 빈도 등에 따른 정책을 설정하게 된다.

### 웹 크롤러의 사용 정책 및 방법

- #### 각 페이지의 상대적인 중요성에 따라
앞서 말했듯이, 웹 크롤러를 사용할 때에 모든 웹 사이트를 탐색하는 것은 상당히 무리가 있다.
탐색할 사이트가 무한에 가까울 정도로 많을 수 있고, 그렇게 찾아낸 페이지가 모두 필요한 페이지가 아닐 가능성이 높다.
이를 방지하기 위해, 페이지를 얼마나 탐색할 것인지, 어떤 중요한 정보를 포함하고, 해당 정보를 포함할 가능성은 어느 정도일지 등에 따라 해당 페이지를 크롤링할지를 결정해야 한다.

- #### 업데이트 빈도에 따라
사이트에 따라 다르겠지만, 포털 사이트를 비롯한 거대한 웹사이트의 경우 그 웹의 컨텐츠가 주기적으로 추가, 수정, 삭제된다.
때문에, 주기적으로 웹사이트를 크롤링하여 해당 정보가 갱신되었는 지 등에 따라 컨텐츠를 최신화할 필요가 있다.

- #### **Robots.txt**에 따라
Robots Exclusion Protocol이라고도 불리며, 해당 텍스트 파일에 따라 어떤 페이지를 크롤링할 지에 대해 결정한다.
웹 크롤러들은 웹페이지를 탐색하기 전에 웹 서버로부터 호스팅 되는 이 Robots.txt 텍스트 파일을 확인하게 된다.
이는 즉, 웹 사이트나 프로그램에 액세스하는 크롤링 봇들에 대해 데이터를 얻을 수 있는 지 없는 지에 대한 규칙을 정해주는 일종의 윤리인 셈이다.

---

[참고]
https://www.cloudflare.com/learning/bots/what-is-a-web-crawler/