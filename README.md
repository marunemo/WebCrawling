웹 크롤링(Web Crawling)
===

> 소프트웨어 프로그램을 통해 웹 사이트에 자동으로 접속하여 해당 데이터를 얻는 기술

## 웹 크롤링의 정의
  월드 와이드 웹(WWW)의 데이터를 조직적이고, 체계적이며, 자동화된 방식으로 탐색하는 기술을 의미한다.
  월드 와이드 웹을 기어다닌다(crawl)는 뉘앙스로 인해 스파이더(spider)라고도 불리며, 자동화된 형태로 통용되기 때문에 스파이더 봇이나 웹 크롤러 봇과 같이 뒤에 봇(bot)이라는 단어를 붙이기도 한다.

## 웹 크롤링의 사용
- ### 검색 엔진으로써의 웹 크롤링
    일반적으로 각종 사이트에서의 검색 엔진에 이용되며, 웹 크롤링을 통해 얻어진 데이터에 검색 알고리즘을 적용하여 사용자의 검색 질의에 대한 응답으로 관련 링크를 제공하는 형태로 주로 사용된다.
    검색 엔진에서의 크롤링 봇은 웹 컨텐츠와 다른 웹 컨텐츠의 인덱스를 크롤링하여, 그 다운로드된 데이터를 복사하여 다른 유저들이 효율적으로 검색을 할 수 있도록 한다.
    조사에 따르면, 전체 인터넷의 40~70% 가량이 검색을 위해 인덱싱되며, 이는 수 십 억 개의 웹페이지에 해당하는 정보량이라고 한다.
    이를 위해서는 웹 크롤러는 사용자가 검색하기 전에 수 억 개에 달하는 웹페이지에서 정보를 모아 이를 검색 색인(search index)에 정리해야 한다.
    크롤러가 웹페이지를 찾으면 시스템에서는 웹 브라우저와 마찬가지로 해당 페이지의 컨텐츠들을 렌더링한다. 이후 키워드나 웹사이트의 최신 컨텐츠와 같은 주요 정보들을 기록하여, 검색 색인에서 이러한 모든 주요 신호를 추적하게 된다.
    검색 색인을 위해 크롤링을 할 때, 정보를 더 상세하게 설정하기 위해 재크롤링을 요청할 수도 있으며, 특정 페이지가 크롤링되지 않도록 하기 위해 'robots.txt'라는 파일을 사용할 수도 있다.
  - #### 사이트맵
      > 사이트에 있는 페이지, 동영상 및 기타 파일과 그 관계에 관한 정보를 제공하는 파일

      Google과 같은 검색엔진은 이 사이트맵 파일을 읽음으로써, 사이트를 더 효율적으로 크롤링할 수 있다. 사이트맵은 웹사이트 소유자가 사이트에서 중요하다고 생각하는 페이지와 파일과 함께, 페이지가 마지막으로 업데이트된 시간, 페이지의 대체 언어 버전과 같은 중요한 관련 정보를 제공한다.\
      일반적으로, 규모가 매우 큰 사이트나 컨텐츠 사이의 연관성 등의 문제로 누락될 가능성이 있는 페이지가 있는 사이트, 미디어 파일이나 뉴스와 같은 특정 컨텐츠 유형을 제공해야 하는 사이트의 경우 사이트맵이 필요할 수 있다. 

## 웹 크롤러의 작동 원리
  웹 크롤러를 작동시키기 위해서는 크롤링할 대상 URL 목록들이 필요하다.
  이때, 이 목록에서 가장 처음 방문할 URL을 시드(seed)라고 부른다.
  이러한 시드를 포함하는 크롤링할 URL 목록을 받는 것으로 시작하여, 해당 URL의 웹사이트를 크롤링하여 데이터를 추출해낸다.
  해당 웹사이트를 크롤링하는 중에 다른 하이퍼링크를 찾아내면, 크롤링할 URL 목록에 해당 링크를 추가한다.
  이러한 과정이 재귀적으로 반복되는데, 그 목록의 컨텐츠들은 무한히 늘어날 수 있다.
  때문에, 크롤링할 URL의 목록을 선택하기 위해, 대상, 목적, 업데이트 빈도 등에 따른 정책을 설정하게 된다.

- ### 크롤링 프론티어(Crawl frontier)
  > 크롤링 중에 탐색할 URL을 저장하고, 이후 탐색할 URL을 추가 및 선택하는 데에 유용한 형태의 데이터 구조

  크롤링 프론티어는 우선순위 큐(priority queue)와 유사한 형태를 띠기도 한다. 일반적으로 이후 설명될 웹 크롤러의 정책을 따르는데, 그 정책에는 다음에 방문해야 하는 페이지, 검색할 페이지의 우선순위, 페이지 방문 빈도 등이 포함될 수 있다.
  이러한 정책은 위에서 말했듯이 어마어마하게 많은 정보량을 처리함에 있어, 웹 크롤링의 효율성을 결정하는 데에 아주 중요한 역할을 한다.
  <p align="center" style="margin: 5% 5%">
    <img src="https://upload.wikimedia.org/wikipedia/commons/b/b2/WebCrawlerArchitecture.png" title="By Lequintanilla - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=56558669" alt="Architecture of a Web crawler" width="80%"></br>
    <b><i>웹 크롤링 구조</i></b>
  </p>
  <!-- By Carlos Castillo = User:ChaTo - Source: PhD. Thesis of Carlos Castillo , image released under GFDL by the author., Public Domain, https://commons.wikimedia.org/w/index.php?curid=32927915 -->
  크롤러는 프론티어에게 다음으로 방문할 페이지를 요청(request)하고, 현재 방문한 페이지에서의 새로운 하이퍼링크를 포함하는 응답(response)을 프론티어에 전송한다.
  이렇게 추가된 하이퍼링크(또는 URL)들은 프론티어에서 각 정책별로 점수가 매겨지게 되고, 이 점수를 기준으로 새로 방문할 페이지의 우선 순위가 결정된다.
  아래 그림에서는 Frontier API/Manager를 통해 웹 크롤러 봇과 통신하여 프론티어로 컨텐츠들을 주고받게 된다. Middlewares에서 Frontier API/Manager와 Backend를 연결하고, Backend에서는 앞서 말한 정책에 따라 프론티어를 관리하게 된다.
  <p align="center" style="margin: 5% 5%">
    <img src="https://upload.wikimedia.org/wikipedia/commons/0/00/Crawler_frontier_architecture.gif" title="By Lequintanilla - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=56558669" alt="Crawler Frontier architecture" width="80%"></br>
    <b><i>크롤링 프론티어 구조</i></b>
  </p>
  <!-- By Lequintanilla - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=56558669 -->

## 웹 크롤러의 사용 정책 및 방법

- ### 각 페이지의 상대적인 중요성에 따라
  앞서 말했듯이, 웹 크롤러를 사용할 때에 모든 웹 사이트를 탐색하는 것은 상당히 무리가 있다.
  탐색할 사이트가 무한에 가까울 정도로 많을 수 있고, 그렇게 찾아낸 페이지가 모두 필요한 페이지가 아닐 가능성이 높다.
  이를 방지하기 위해, 페이지를 얼마나 탐색할 것인지, 어떤 중요한 정보를 포함하고, 해당 정보를 포함할 가능성은 어느 정도일지 등에 따라 해당 페이지를 크롤링할지를 결정해야 한다. 다음 4개의 방법은 크롤링에서 시드를 효율적으로 추출하는 예시이다.

  - #### 팔로우 링크 제한 방식(Restricting followed links)
    HTML 문서가 아닌 모든 MIME 유형은 무시하는 방식이다. 다르게 말하면, 크롤러가 웹 서버로부터 전체 리소스를 다운로드받기 전에, http head에 대한 get 방식의 요청으로 html 리소스만을 받아오는 것이다. .html, .htm, .asp, .aspx, .php, .jsp, .jspx과 같은 파일 확장자나 "/"(slash)로 끝나는 URL의 리소스만을 요청하는 방식으로 탐색할 페이지 수를 줄일 수 있다. 다만, 이 경우에 많은 페이지들이 의도치 않게 누락될 수 있다.

  - #### URL 정규화(URL normalization)
    탐색할 다양한 URL들의 포맷을 하나로 통일하는 방식이다. 일반적으로, 주소를 모두 소문자로 변환하거나, .이나 ..등을 통한 상대 위치 표현식에서 절대 위치 표현식으로 변경하는 방법을 사용한다. 비어있지 않은 경로(path) 중에서 만약 해당 경로가 현재 URL에서 "/"(slash)만 추가되는 URL로 리디렉션된다면, 디렉토리(폴더)로 간주하고 맨 뒤에 "/"를 추가하기도 한다. 때로는 만약 ip주소가 도메인 이름으로 매핑될 때, 도메인주소로 변환시키는 경우도 포함한다. [자세한 내용은 다음 문서를 참고한다](https://en.wikipedia.org/wiki/URI_normalization).

  - #### 경로 확장 크롤링(Path-ascending crawling)
    크롤링하고자 하는 URL이 주어졌을 때, 최하위 디렉토리(폴더)의 파일부터 최상위 디렉토리인 "/"까지의 모든 디렉토리를 시드로 하여 상위 디렉토리로 점차 올라가며 크롤링하는 방식이다. Viv Cothey(2004)는 이 방식이 고립된 자원이나 일반적인 크롤링으로 발견할 수 없던 인바운드 링크를 찾는 데에 매우 효과적인 방법임을 발견했다.

  - #### Focused crawling
    크롤링 프런티어의 우선순위를 보다 신중하게 정하고 하이퍼링크 탐색 프로세스를 관리함으로써 특정 속성을 만족시키는 방식이다.
    .kr로 끝나는 도메인이나 특정 주제를 포함하는 페이지에서만 크롤링을 하는 것을 예시로 들 수 있다. 이를 통해 불필요한 페이지를 다운로드하는 데에 사용되는 자원을 최소화하고, 대신 그 자원들을 원하는 주제를 위해 효율적으로 사용할 수 있게 된다.\
    이를 위해서는 크롤러가 페이지들을 다운로드하기 전에 해당 페이지가 주제와 관련되어 있을 가능성을 예측해야 한다. 가장 가능성 있는 방법은 초창기 웹 크롤러에서 Pinkerton에 의해 제안된 링크의 앵커 텍스트(anchor text)를 참고하는 것이다(html에서 `<a>`로 사용되는 태그다). 이후에는 이미 방문했던 페이지들의 컨텐츠들을 통해 쿼리와 방문되지 않은 페이지 사이의 유사성을 추론하는 방법이 제안되었다. 때문에, Forced crawling의 성능은 특정 주제에 대한 관련 링크들의 다양성과 시드로 주어지는 웹 검색 엔진의 성능에 의존한다.

- ### 업데이트 빈도에 따라
  사이트에 따라 다르겠지만, 포털 사이트를 비롯한 거대한 웹사이트의 경우 그 웹의 컨텐츠가 주기적으로 추가, 수정, 삭제된다.
  때문에, 주기적으로 웹사이트를 크롤링하여 해당 정보가 갱신되었는 지 등에 따라 컨텐츠를 최신화할 필요가 있다.

- ### **Robots.txt**에 따라
  Robots Exclusion Protocol이라고도 불리며, 해당 텍스트 파일에 따라 어떤 페이지를 크롤링할 지에 대해 결정한다.
  웹 크롤러들은 웹페이지를 탐색하기 전에 웹 서버로부터 호스팅 되는 이 robots.txt 텍스트 파일을 확인하게 된다.
  robots.txt 파일은 주로 사이트의 크롤러 트래픽을 관리하기 위해 사용되며, 그 중에서도 주로 사이트에 대한 요청으로 인해 서버가 오버로드되는 것을 방지하기 위해 사용된다.
  하지만 이 파일은 웹 사이트나 프로그램에 액세스하는 크롤링 봇들에 대해 데이터를 얻을 수 있는 지 없는 지에 대한 규칙을 정해주지만, 크롤러를 강제로 통제할 수 없기 때문에 어디까지나 일종의 윤리인 셈이다.

  - #### 서버 개발자의 관점에서의 robots.txt
    웹 크롤러가 웹 컨텐츠들을 인덱싱하기 위해서는 서버에 리소스를 요청(request)해야 한다.
    그러나 요청되는 데이터의 양이 너무 많거나 리소스를 너무 빈번하게 요청하게 되면, 서버에 과부하가 걸리거나 서버에 부담되는 대역폭에 따른 비용(bandwidth cost)이 커지게 된다.
    따라서 서버나 웹을 운영하는 입장에서는 이러한 상황을 막기 위해, 일반적으로 웹 크롤러의 컨텐츠 인덱싱을 너무 자주 허용하지 않도록 설정한다.
    때문에 사람들에 맞는 메세지를 제공하거나 페이지 검색 성능을 강화하기 위해, robots.txt 파일에 **"disallow"** 태그를 추가하는 등의 방법으로 검색 엔진과 같은 웹 크롤러가 해당 페이지를 크롤링하는 것을 막게 된다.

    게다가 몇몇 개발자가 기업의 입장에서는 일부 웹페이지를 크롤링하는 것을 꺼려할 수 있다.
    이 경우에는 **noindex**로 색인 생성을 차단하거나 비밀번호로 페이지를 보호하는 방식을 사용하여, 사이트가 검색 색인등의 목적으로 크롤링되는 것을 막을 수 있다.
    하지만 이러한 robots.txt 파일을 사용하여 페이지가 크롤링되는 것은 차단될 수 있지만, URL을 통한 접근은 여전히 숨겨지지 않는다는 점을 유의해야 한다.

    단, robots.txt 명령어는 어디까지나 지침일 뿐 크롤러의 동작을 강제할 수 없기 때문에, 몇몇 크롤러는 이러한 지침을 준수하지 않을 수 있다. 게다가, robots.txt 파일의 지침을 따르더라도 웹 크롤러에 따라 특정 지침을 이해하지 못하거나 다르게 해석할 수 있으므로, 다양한 웹 크롤러에 적용될 수 있는 적절한 구문을 이해해야 한다. robots.txt 파일에서 허용되지 않은 페이지라도 다른 사이트를 통해 연결된 경우에도, 해당 페이지로 크롤러가 접근할 수 있다. 그렇기 때문에 웹 크롤러로부터 정보를 안전하게 보호하기 위해서는 서버의 파일을 비공개로 전환하여 비밀번호로 보호하는 등의 다른 차단 방법을 사용해야 한다.

  - #### 웹 스크래핑
    > 크롤러를 사용하여 허가 없이 웹사이트의 컨텐츠를 다운로드하는 경우

    웹 스크래핑은 데이터 스크래핑이나 컨텐츠 스크래핑이라는 용어로도 불린다. 허가를 받지 않는 방식이기 때문에 정보 윤리의 관점에서도 문제가 되지만, 일반적으로 웹 스크래핑은 해당 컨텐츠를 악의적인 목적으로 이용하려는 경우가 많기 때문에 주의해야 한다.
    페이지에 연결된 링크들을 따라가며 컨텐츠를 받아오는 웹 크롤링과 달리, 웹 스크래핑은 특정 페이지나 웹사이트만을 추적하는 형태로도 나타난다.
    뿐만 아니라, 웹 크롤러는 robots.txt 파일의 규칙을 준수하며 웹 서버에 부담이 되지 않는 선에서 요청을 하지만, 웹 스크래핑은 이러한 웹 서버에 대한 규칙과 영향을 무시하는 경우도 포함한다.

---

## [**참고 자료**]
### **cloudflare에서 설명하는 웹 크롤러**
https://www.cloudflare.com/learning/bots/what-is-a-web-crawler/ </br>
### **웹 크롤러와 크롤링 프론티어에 대한 간단한 참고**
https://en.wikipedia.org/wiki/Web_crawler </br>
https://en.wikipedia.org/wiki/Crawl_frontier </br>
### **구글에서 말하는 검색 엔진을 위한 웹 크롤러**
https://www.google.com/intl/ko/search/howsearchworks/crawling-indexing/ </br>
https://developers.google.com/search/docs/advanced/sitemaps/overview?hl=ko&visit_id=637819895250469830-3265437114&rd=1 </br>
https://developers.google.com/search/docs/advanced/robots/intro </br>
https://developers.google.com/search/docs/advanced/crawling/block-indexing